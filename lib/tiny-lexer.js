const log = console.log.bind (console)
const StringsInto = map => new Proxy ({}, { get:($,k) => (map [k] = k, k) })

// A tiny-lexer based tokenizer for HTML5
// =======================================

// ### The tokens

// 'space' should be renamed; it is not clear that this is within-tags

const tokenTypes = {}
const T = tokenTypes
const {
  attributeName, 
  attributeAssign, 
  attributeValueStart, 
  attributeValueData, 
  attributeValueEnd, 
  tagSpace, // white space (or slashes) in between attributes and/ or
  commentStart, 
  commentStartBogus, 
  commentData, 
  commentEnd, 
  commentEndBogus, 
  startTagStart, 
  endTagStart, 
  tagEnd, 
  tagEndClose, 
  charRefDecimal, 
  charRefHex, 
  charRefNamed, 
  unescaped, 
  data, 
  rcdata, 
  rawtext, 
  plaintext, 
} = StringsInto (tokenTypes)


//### The grammar

const STARTTAG_START = '<[a-zA-Z][^>/\t\n\f ]*'
const ENDTAG_START = '</[a-zA-Z][^>/\t\n\f ]*'
const CHARREF_DEC = '&#[0-9]+;?'
const CHARREF_HEX = '&#[xX][0-9A-Fa-f]+;?'
const CHARREF_NAMED = '&[A-Za-z][A-Za-z0-9]*;?'
const ATTNAME = '.[^>/\t\n\f =]*' /* '[^>/\t\n\f ][^>/\t\n\f =]*' */
const ATT_UNQUOT = '[^&>\t\n\f ]+'
const UNQUOT_END = '(?=[>\t\n\f ])'
const DOCTYPE_START = '<![Dd][Oo][Cc][Tt][Yy][Pp][Ee]'

// The below generated by preprocessing the list of named character references;
// Legacy charrefs may occur without terminating semicolon but not as a prefix
// of a known named reference. 

const CHARREF_CONTD = '&(?:copysr|centerdot|divideontimes|[gl]t(?:quest|dot|cir|cc)|[gl]trPar|gtr(?:dot|less|eqqless|eqless|approx|arr|sim)|ltr(?:i|if|ie|mes)|ltlarr|lthree|notin(?:dot|E|v[abc])?|notni(?:v[abc])?|parallel|times(?:bar|d|b));'
const CHARREF_LEGACY = '&(?:[AEIOUYaeiouy]?acute|[AEIOUaeiou](?:grave|circ|uml)|y?uml|[ANOano]tilde|[Aa]ring|[Oo]slash|[Cc]?cedil|brvbar|curren|divide|frac(?:12|14|34)|iquest|middot|plusmn|(?:AE|ae|sz)lig|[lr]aquo|iexcl|micro|pound|THORN|thorn|times|COPY|copy|cent|macr|nbsp|ord[fm]|para|QUOT|quot|sect|sup[123]|AMP|amp|ETH|eth|REG|reg|deg|not|shy|yen|GT|gt|LT|lt);?'

const grammar = 
{ data: [
  [ STARTTAG_START, T.startTagStart,        startTag      ],
  [ ENDTAG_START,   T.endTagStart,         'beforeAtt'    ],
//[ DOCTYPE_START,  T.doctype_start,       'beforeName'   ], // before doctype name
  [ '<!--',         T.commentStart,        'commentStart' ],
  [ '<[/!?]',       T.commentStartBogus,   'bogusComment' ],
  [ '[^<&]+',       T.data                                ],
  [ '<',            T.unescaped                           ],
  [ '',             T.data,                 charRefIn     ]],

rawtext: [
  [ ENDTAG_START,   maybeEndTagT,           maybeEndTag   ],
  [ '.[^<]*',       T.rawtext                             ]],

rcdata: [
  [ ENDTAG_START,   maybeEndTagT,           maybeEndTag   ],
  [ '<',            T.unescaped                           ],
  [ '[^<&]+',       T.rcdata                              ],
  [ '',             T.rcdata,               charRefIn     ]],

plaintext: [
  [ '.+',           T.plaintext                           ]],

charRef: [
  [ CHARREF_DEC,    T.charRefDecimal,       context       ],
  [ CHARREF_HEX,    T.charRefHex,           context       ],
  [ CHARREF_CONTD,  T.charRefNamed,         context       ],
  [ CHARREF_LEGACY, legacyCharRefT,         context       ],
  [ CHARREF_NAMED,  T.charRefNamed,         context       ],
  [ '&',            T.unescaped,            context       ]],

beforeAtt: [
  [ '>',            T.tagEnd,               content       ],
  [ '/>',           T.tagEndClose,          content       ],
  [ '[\t\n\f ]+',   T.tagSpace,                           ],
  [ '/+(?!>)',      T.tagSpace,                           ], // TODO, test / check with spec
  [ ATTNAME,        T.attributeName,       'afterAttName' ]],

afterAttName: [
  [ '>',            T.tagEnd,               content       ],
  [ '/>',           T.tagEndClose,          content       ],
  [ '=[\t\n\f ]*',  T.attributeAssign,     'attValue'     ],
  [ '/+(?!>)',      T.tagSpace,            'beforeAtt'    ],
  [ '[\t\n\f ]+',   T.tagSpace                            ],
  [ ATTNAME,        T.attributeName                       ]],

attValue: [ // 'equals' has eaten all the space
  [ '>' ,           T.tagEnd,               content       ],
  [ '"' ,           T.attributeValueStart, 'doubleQuoted' ],
  [ "'" ,           T.attributeValueStart, 'singleQuoted' ],
  [ '',             T.attributeValueStart, 'unquoted'     ]],

unquoted: [
  [ ATT_UNQUOT,     T.attributeValueData                  ],
  [ UNQUOT_END,     T.attributeValueEnd,   'beforeAtt'    ],
  [ '',             T.attributeValueData,   charRefIn     ]],

doubleQuoted: [
  [ '[^"&]+',       T.attributeValueData                  ],
  [ '"',            T.attributeValueEnd,   'beforeAtt'    ],
  [ '',             T.attributeValueData,   charRefIn     ]],

singleQuoted: [
  [ "[^'&]+",       T.attributeValueData                  ],
  [ "'",            T.attributeValueEnd,   'beforeAtt'    ],
  [ '',             T.attributeValueData,   charRefIn     ]],

bogusComment: [
  [ '[^>]+',        T.commentData,         'bogusComment' ],
  [ '>',            T.commentEndBogus,      content       ]],

commentStart: [
  [ '-?>',          T.commentEnd,           content       ],
  [ '--!?>',        T.commentEnd,           content       ],
  [ '--!',          T.commentData,         'comment'      ],
  [ '--?',          T.commentData,         'comment'      ],
  [ '[^>-][^-]*',   T.commentData,         'comment'      ]],

comment: [
  [ '--!?>',        T.commentEnd,           content       ],
  [ '--!'  ,        T.commentData                         ],
  [ '--?'  ,        T.commentData                         ],
  [ '[^-]+',        T.commentData                         ]]
}


// Additional state management, to
//  supplement the grammar/ state machine. 

const content_map = 
  { style: 'rawtext'
  , script: 'rawtext'
  , xmp: 'rawtext'
  , iframe: 'rawtext'
  , noembed: 'rawtext'
  , noframes: 'rawtext'
  , textarea: 'rcdata'
  , title: 'rcdata'
  , plaintext: 'plaintext'
  //, noscript: 'rawtext' // if scripting is enabled in a UA
  }

function PrivateState (input) {
  this.content = 'data' // one of { data, rcdata, rawtext, unquoted, doubleQuoted, singleQuoted }
  this.context = 'data' // likewise
  this.tagName // the last seen 'startTag-start' name 
  this.position = 0
  this.input = input
}

function startTag (_, chunk) {
  let tagName = chunk.substr (1)
  this.tagName = tagName
  this.content = tagName in content_map ? content_map[tagName] : 'data'
  return 'beforeAtt'
}

function content () {
  return this.content
}

function context () {
  return this.context
}

// From the spec;
// "If the character reference was consumed as part of an attribute, 
// and the last character matched is not a U+003B SEMICOLON character (;), 
// and the next input character is either a U+003D EQUALS SIGN character (=) or an ASCII alphanumeric, 
// then, for historical reasons, flush code points consumed as a character reference and switch to the return state."

function legacyCharRefT () {
  const x = this.context, c = this.input[this.position]
  if ((x === 'unquoted' || x === 'doubleQuoted' || x === 'singleQuoted') && /[a-zA-Z0-9=]/.test(c)) {
    return T.attributeValueData
  }
  return T.charRefNamed
}

function maybeEndTagT (_, chunk) {
  if (chunk.substr (2) === this.tagName) {
    this.content = 'data'
    return T.endTagStart
  }
  else return this.content // TODO careful, this is a token type, not a state!
}

function maybeEndTag (symbol, chunk) {
  if (chunk.substr (2) === this.tagName) {
    this.content = 'data'
    return 'beforeAtt'
  }
  else return symbol
}

function charRefIn (symbol, chunk) {
  this.context = symbol
  return 'charRef'
}


// The compiler and runtime
// ------------------------

function State (table, name) {
  this.name = name
  this.regex = new RegExp ('(' + table.map (fst) .join (')|(') + ')', 'sgy')
  this.edges = table.map (compileRow (name))
}

function compile (grammar) {
  const compiled = {}
  for (let state_name in grammar)
    compiled [state_name] = new State (grammar [state_name], state_name)
  return compiled
}

function fst (row) {
  return Array.isArray (row) ? row [0] || '.{0}'
    : 'if' in row ? row.if : '.{0}'
}

function compileRow (symbol) {
  return function (row) {
    let r, emit, goto
    if (Array.isArray) [r = '.{0}', emit, goto = symbol] = row
    else ({ if:r = '.{0}', emit, goto = symbol } = row )
    const g = typeof goto === 'function' ? goto : (symbol, data) => goto
    const e = typeof emit === 'function' ? wrapEmit (emit) : (symbol, data) => [emit, data]
    return { emit:e, goto:g }
  }
}

function wrapEmit (fn) { return function (type, data) {
  return [fn.call (this, type, data), data]
}}


// The Lexer runtime
// -----------------

function TinyLexer (grammar, start, CustomState = Object) {
  const states = compile (grammar)

  this.tokenize = function (input, position = 0, symbol = start) {
    const custom = new CustomState (input, position, symbol)
    const stream = tokenize (input, custom, position, symbol)
    stream.state = custom
    return stream
  }

  function *tokenize (input, custom, position, symbol) {
    do if (!(symbol in states))
      throw new Error (`Lexer: no such symbol: ${symbol}.`)

    else {
      const state = states [symbol]
      const regex = state.regex
      const match = (regex.lastIndex = position, regex.exec (input))

      if (!match){
        if (position !== input.length)
          throw new SyntaxError (`Lexer: invalid input at index ${position} in state ${symbol} before ${input.substr (position, 12)}`)
        return
      }

      let i = 1; while (match [i] == null) i++
      const edge = state.edges [i-1]
      const token = edge.emit.call (custom, symbol, match[i])
      symbol = edge.goto.call (custom, symbol, match[i])
      position = regex.lastIndex
      Object.assign (custom, { symbol, position })
      yield token
    }

    while (position <= input.length)
  }
}

// Wrapping it all up
// ------------------

const tokenize = new TinyLexer (grammar, 'data', PrivateState) .tokenize
module.exports = { lexemes:tokenize, chunks:tokenize, tokenTypes }